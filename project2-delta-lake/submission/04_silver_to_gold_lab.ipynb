{"cells":[{"cell_type":"markdown","source":["# Silver to Gold Step"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc99d1a4-b5ef-4c19-80a5-d0f94969c09c"}}},{"cell_type":"markdown","source":["## Notebook Objective\n\nIn this notebook you:\n1. Harden the Silver to Gold step we wrote in the previous notebook using the composable functions in the operations file."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c12ddc7e-21eb-4bd7-b96d-1d9ecf560649"}}},{"cell_type":"markdown","source":["## Step Configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b92e8eb-03b8-4035-978a-d2a4ee62b1eb"}}},{"cell_type":"code","source":["%run ./includes/configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"246b43ea-aec2-4d97-a4e1-f3658803a1bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[23]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[23]: DataFrame[]"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"No running streams.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["No running streams.\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Import Operation Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab779644-30f3-487e-88ed-691188079e4b"}}},{"cell_type":"code","source":["%run ./includes/main/python/operations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed320bc5-385f-4c82-9be6-358351746dda"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Start Streams\n\nBefore we add new streams, let's start the streams we have previously engineered.\n\nWe will start two named streams:\n\n- `write_raw_to_bronze`\n- `write_bronze_to_silver`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8e40f52-da3a-4045-989b-c67273335f93"}}},{"cell_type":"markdown","source":["### Current Delta Architecture\n\nNext, we demonstrate everything we have built up to this point in our\nDelta Architecture.\n\nAgain, we do so with composable functions included in the\nfile `includes/main/python/operations`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"438d7610-2697-42c8-aa43-e0d8a99860b1"}}},{"cell_type":"code","source":["rawDF = read_stream_raw(spark, rawPath)\ntransformedRawDF = transform_raw(rawDF)\nrawToBronzeWriter = create_stream_writer(\n    dataframe=transformedRawDF,\n    checkpoint=bronzeCheckpoint,\n    name=\"write_raw_to_bronze\",\n    partition_column=\"p_ingestdate\",\n)\nrawToBronzeWriter.start(bronzePath)\n\nbronzeDF = read_stream_delta(spark, bronzePath)\ntransformedBronzeDF = transform_bronze(bronzeDF)\nbronzeToSilverWriter = create_stream_writer(\n    dataframe=transformedBronzeDF,\n    checkpoint=silverCheckpoint,\n    name=\"write_bronze_to_silver\",\n    partition_column=\"p_eventdate\",\n)\nbronzeToSilverWriter.start(silverPath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60083200-fefd-488c-b3f1-e1844dc50948"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[35]: <pyspark.sql.streaming.StreamingQuery at 0x7fb52162ebb0>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[35]: <pyspark.sql.streaming.StreamingQuery at 0x7fb52162ebb0>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Exercise:** Harden the Silver to Gold step that we created in the previous notebook.\n\nNow that you have seen the pattern, fill out the following code block to complete this step.\n\nüíÅüèª‚Äç‚ôÄÔ∏èRemember to use `mode=\"complete\"` with streaming aggregate tables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c5cb7c5-83ab-4efc-8771-c73b2b51c808"}}},{"cell_type":"code","source":["tableName = \"/aggregate_heartrate\"\ntableCheckpoint = goldCheckpoint + tableName\ntablePath = goldPath + tableName\n \nsilverDF = read_stream_delta(spark, silverPath)\n\ntransformedSilverDF = transform_silver_mean_agg(silverDF)\n\nsilverToGoldAggWriter = create_stream_writer(\n    dataframe=transformedSilverDF,\n    checkpoint=tableCheckpoint,\n    name=\"write_silver_to_gold\",\n    mode=\"complete\",\n)\nsilverToGoldAggWriter.start(tablePath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21e2b207-85c6-4d0e-8868-6cbb6973c05f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[36]: <pyspark.sql.streaming.StreamingQuery at 0x7fb520589220>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[36]: <pyspark.sql.streaming.StreamingQuery at 0x7fb520589220>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## **Exercise:** Show all running streams"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b03f7ab0-a5a7-45ca-b994-d008e76a2ebb"}}},{"cell_type":"code","source":["\nfor stream in spark.streams.active:\n    print(stream.name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ed7bc32-b8b4-452a-b2c3-649e4ab1528c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"write_bronze_to_silver\nwrite_raw_to_bronze\nwrite_silver_to_gold\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["write_bronze_to_silver\nwrite_raw_to_bronze\nwrite_silver_to_gold\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Stop All Streams\n\nIn the next notebook, we will take a look at schema enforcement and evolution with Delta Lake.\n\nBefore we do so, let's shut down all streams in this notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3619c85a-a250-4e77-9853-8c0e9f763d3e"}}},{"cell_type":"code","source":["stop_all_streams()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66803b03-3c35-42c8-a47a-89b7400e6646"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[38]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[38]: True"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d1317bb-5b53-49f0-b04f-9110ec3dfa75"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"04_silver_to_gold_lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3719025789001888}},"nbformat":4,"nbformat_minor":0}
